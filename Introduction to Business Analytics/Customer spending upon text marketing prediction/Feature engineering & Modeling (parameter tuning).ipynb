# -*- coding: utf-8 -*-

# =============================================================================
# Loading Packages, Setting Preferences
# =============================================================================
import requests as rq
from io import BytesIO
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import scipy
from scipy import stats
from scipy.stats import boxcox
from scipy.special import inv_boxcox
from scipy.stats import shapiro
import seaborn as sns
import statsmodels.api as sm
import pylab as py
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.compose import TransformedTargetRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

#these improve data readability on my machine, but YMMV
pd.set_option('display.max_columns', None)
pd.set_option('expand_frame_repr', False)

#setting random seed
np.random.seed(42)

# =============================================================================
# Loading Data
# =============================================================================
url=r'https://github.com/jentlapalm/Homework-4B/blob/main/HW4.xlsx?raw=true'
data = rq.get(url).content
df = pd.read_excel(BytesIO(data))

# =============================================================================
# Displaying Column Descriptions
# =============================================================================
url2=r'https://github.com/jentlapalm/Homework-4B/blob/main/column_data.xlsx?raw=true'
data2 = rq.get(url2).content
col_info = pd.read_excel(BytesIO(data2))
print(col_info)

# =============================================================================
# Describe and Visualize Data 
# =============================================================================

#describing data
for i in df.columns:
    print(df[i].describe())

#there are no null values
df.isnull().sum()

#plotting distributions of non-binary variables
colnames=['Freq','last_update_days_ago','1st_update_days_ago','Spending']
for i in colnames:
    _, bins, _ = plt.hist(df[i], 20, density=1, alpha=0.5)
    mu, sigma = scipy.stats.norm.fit(df[i])
    best_fit_line = scipy.stats.norm.pdf(bins, mu, sigma)
    plt.plot(bins, best_fit_line)
    plt.title(i)
    plt.show()

#notes: 
#Freq is right-skew, last_update is weird, 1st update is left-skew
#Spending is right-skew

#correlations heatmap
corr=df.iloc[:,1:26].corr()
corr["Spending"]
#spending strong correlates: Freq, last_update (neg), Purchase
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1, vmax=1)

#did anyone who did not get a purchase spend money?
x=df.loc[(df['Purchase']==0) & (df['Spending']>0)]
sum(x.iloc[:,1])#369 people spent money but did not do test purchase
sum(x['Spending'])#they spend a total of $31

# =============================================================================
# Feature Engineering, Transform, and Scale Data
# =============================================================================
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from scipy.stats import boxcox
from scipy.special import inv_boxcox
import pandas as pd
import numpy as np

#copying df
df_fe=df.copy()

#account age variable
df_fe['account_age']=df['1st_update_days_ago']-df['last_update_days_ago']
#freq transformation
df_fe['Freq']=np.log(df['Freq']+0.1)
#spending transformation: using square root because boxcox tough with k-fold
df_fe['Spending'],_=boxcox(df['Spending']+0.1)
# df_fe['Spending']=np.sqrt(df['Spending'])
#dropping sequence num. column, unneccesary
df_fe=df_fe.drop(columns=['sequence_number'])

#scaling all but target variable
original_colnames=df_fe.columns

col_names=['US', 'source_a', 'source_c', 'source_b', 'source_d', 'source_e','source_m', 'source_o', 'source_h', 'source_r', 'source_s', 'source_t', 
 'source_u', 'source_p', 'source_x', 'source_w', 'Freq',
       'last_update_days_ago', '1st_update_days_ago', 'Web order',
       'Gender=male', 'Address_is_res', 'Purchase', 'account_age']

ct = ColumnTransformer([('attributes', StandardScaler(), col_names)], remainder='drop')
df_fe[col_names]=ct.fit_transform(df_fe)

# function to undo y transformation for boxcox
def y_func(ytrain):
    x,_=boxcox(ytrain+0.1)
    return x

def y_xform(ypred):
    a=inv_boxcox(ypred,-0.02965434077579322)-.1
    return np.round(a,4)

def square(ypred):
    return ypred**2

"""THOUGHT PROCESS, Spending Untransform Function"""
#graphing variables vs spending to look for more relationships
for i in df.loc[:, df.columns != 'Spending'].columns:
    plt.figure()
    plt.scatter(df[i],df['Spending'])
    plt.xlabel(i)
    plt.ylabel('Spending')
    plt.show()

#Data Transformation Comparisons for 'Freq'
#raw data for comparison
sm.qqplot(df['Freq'],line='45',fit=True)
plt.title('raw data')
#boxcox transformation
x,_=boxcox(df['Freq']+.1) 
sm.qqplot(x, line ='45',fit=True)
plt.title('boxcox')
#log(data+.1) transformation
sm.qqplot(np.log(df['Freq']+.1),line='45',fit=True)
plt.title('log(data+0.1)')

#working on 1st update days ago: none of the transformations are a big improvement
#raw data
sm.qqplot(df['1st_update_days_ago'],line='45',fit=True)
#trying boxcox
x2,_=boxcox(df['1st_update_days_ago'])
sm.qqplot(x2,line='45',fit=True)
#trying 2nd power
sm.qqplot(df['1st_update_days_ago']**2,line='45',fit=True)

#working on Spending
sm.qqplot(df['Spending'],line='45',fit=True)
plt.title('Spending')
#trying boxcox
x3,_=boxcox(df['Spending']+0.1)
sm.qqplot(x3,line='45',fit=True)
plt.title('boxcox')
#trying log
sm.qqplot(np.log(df['Spending']+0.1),line='45',fit=True)
plt.title('log(data+0.1)')
#trying sq root
sm.qqplot(np.sqrt(df['Spending']),line='45',fit=True)
plt.title('Square Root')
#trying ln(1+y)
sm.qqplot(np.log1p(df['Spending']),line='45',fit=True)


#Inverting BoxCox xform
Spending_bxcx,lmbda=boxcox(df['Spending']+0.1)
Spending_bxcx_inv=inv_boxcox(Spending_bxcx,lmbda)
round(df['Spending']-Spending_bxcx_inv+0.1,5)

#who is most likely to spend?
p=df[['US','Freq','Web order','Gender=male','Address_is_res', 'Spending']].sort_values('Spending',ascending=False)
pp=p.head(500)
sum(pp['Spending'])/sum(p['Spending'])

for i in ['US','Freq','Web order','Gender=male','Address_is_res', 'Spending']: 
    print(i,sum(pp[i]))
    
# =============================================================================
# Testing Models on FE Data
# =============================================================================
from sklearn.model_selection import train_test_split # Split validation class
X=df_fe.loc[:, df_fe.columns != 'Spending']# no spending
y=df_fe['Spending']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)

#k-NN
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn import neighbors
knn = neighbors.KNeighborsRegressor(n_neighbors=5,
                                    weights='distance',
                                    p=3, 
                                    metric='minkowski')
knn = knn.fit(X_train, y_train)  
#predict
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

#Using Transformed Target Regressor to invert y transformation
ttr_knn = TransformedTargetRegressor(regressor=knn, inverse_func=y_xform, check_inverse=False)

# evaluate model
cv = KFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_val_score(ttr_knn, X, y, scoring='neg_root_mean_squared_error', cv=cv)
print('Mean RMSE: %.3f, sd: %.3f' % (abs(np.mean(scores)), np.std(scores)))

ttr_knn.get_params()

# =============================================================================
# k-NN Parameter Tuning
# =============================================================================
from sklearn.model_selection import GridSearchCV
k_list = list(range(1, 10))
gs_knn = GridSearchCV(estimator=ttr_knn,
                    param_grid=[{'regressor__n_neighbors':k_list,
                                'regressor__weights' : ['uniform','distance'],
                                'regressor__algorithm': ['auto', 'ball_tree','kd_tree', 'brute'],
                                'regressor__p':[1,2]}],
                                scoring='neg_root_mean_squared_error',cv=10)

gs_knn = gs_knn.fit(X_train,y_train)
print(gs_knn.best_score_)
print(gs_knn.best_params_)
print(gs_knn.best_estimator_)
# =============================================================================
# Decision Tree Tuning
# =============================================================================
#Decision Tree
from sklearn.tree import DecisionTreeRegressor
tree = DecisionTreeRegressor(max_depth=5)

#Using Transformed Target Regressor to invert y transformation
ttr_tree = TransformedTargetRegressor(regressor=tree, inverse_func=y_xform, check_inverse=False)

# evaluate model
cv = KFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_val_score(ttr_tree, X, y, scoring='neg_root_mean_squared_error', cv=cv)
print('Mean RMSE: %.3f, sd: %.3f' % (abs(np.mean(scores)), np.std(scores)))

#Grid Search CV
gs_tree = GridSearchCV(estimator=ttr_tree,
                    param_grid=[{'regressor__max_depth': [3,4,5,6,7,8,9,10,None],
                                'regressor__min_samples_leaf':[1,2,3,4,5,6],
                              'regressor__min_samples_split':[2,3,4,5,6],
                              'regressor__min_impurity_decrease' : [0,0.1,0.001,0.0001]}],
                    scoring='neg_root_mean_squared_error', cv=10)

gs_tree = gs_tree.fit(X_train,y_train)
print(abs(gs_tree.best_score_))
print(gs_tree.best_params_)
print(gs_tree.best_estimator_)

#Linear Regression
from sklearn.linear_model import LinearRegression

slr = LinearRegression() 
slr.fit(X_train, y_train)  
y_train_pred = slr.predict(X_train) 
y_test_pred = slr.predict(X_test) 

#RMSE
print('RMSE train: %.3f, test: %.3f' % (  
        mean_squared_error(y_xform(y_train), y_xform(y_train_pred), squared=False),
        mean_squared_error(y_xform(y_test), y_xform(y_test_pred),squared=False))) 
#MAE
print('MAE train: %.3f, test: %.3f' % (
        mean_absolute_error(y_xform(y_test), y_xform(y_test_pred)),
        mean_absolute_error(y_xform(y_test), y_xform(y_test_pred)))) 

